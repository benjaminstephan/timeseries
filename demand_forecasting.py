import pandas as pd
import numpy as np

from datetime import datetime

from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import ExpSineSquared
from sklearn.metrics import mean_absolute_error

from sklearn.ensemble import RandomForestRegressor

from keras.models import Sequential
from keras.layers import Dense, Conv1D, Flatten
from keras.regularizers import l2


class NeuralNetwork(Sequential):
    """"
    A wrapper for keras Sequential models that reshapes the input data into the right format for 1D conv layers.
    This allows for shared API between sklearn models as well as this model.
    """

    def __init__(self):
        super(Sequential, self).__init__()

    @staticmethod
    def _reshape_data(x):
        x = x[:, :, np.newaxis]
        return x

    def fit(self, x, y, epochs=50, verbose=0):
        x = self._reshape_data(x)
        super(Sequential, self).fit(x, y, epochs=epochs, verbose=verbose)

    def predict(self, x):
        x = self._reshape_data(x)
        return super(Sequential, self).predict(x)


def get_weekly_transportation_volumes(raw_data_path):
    """
    Reads the specified csv file and transforms it to a pd.DataFrame that contains the last day of a given week as well
    as the standardized total transportation volume for that week.

    :rtype: pd.DataFrame
    :param raw_data_path: path to a csv that contains 2 rows, where to first row contains dates in ISO 8601 %G%V format and
    the second row contains the total transportation volumes for these weeks
    :return: pd.DataFrame
    Contains columns:
        - ds: datetime objects indicating the last day (sunday) of the corresponding week
        - y: standardized total transportation volume of the week
    """

    # read the raw data
    data = pd.read_csv(raw_data_path, header=None)

    # transpose the dataframe and name the columns
    data = data.T
    data = data.rename(columns={0: 'ds', 1: 'y'})

    # add ISO 8601 weekday as a decimal number where 7 is Sunday
    # refer to https://docs.python.org/3/library/datetime.html#datetime.datetime.isocalendar for more details
    data['ds'] = data['ds'].astype(str) + '7'

    # create datatime objects from strings
    # %G refers to ISO 8601 year, %V refers to ISO 8601 month, %u refers to ISO 8601 weekday
    # refer to https://docs.python.org/3/library/datetime.html#datetime.datetime.isocalendar for more details
    data['ds'] = data['ds'].apply(lambda x: datetime.strptime(x, '%G%V%u'))

    # standardize data
    data['y'] = (data['y'] - np.mean(data['y'])) / np.std(data['y'])

    return data


def get_gaussian_process():
    kernel = ExpSineSquared(length_scale_bounds=(0.5, 100), periodicity=52, periodicity_bounds=(10, 100))
    gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=5, normalize_y=False, n_restarts_optimizer=10)
    return gaussian_process


def get_neural_network():
    model = NeuralNetwork()
    model.add(Conv1D(8, kernel_size=3, input_shape=(20, 1), kernel_regularizer=l2(1)))
    model.add(Flatten())
    model.add(Dense(16, activation='relu', kernel_regularizer=l2(1)))
    model.add(Dense(8, activation='relu', kernel_regularizer=l2(1)))
    model.add(Dense(4, activation='relu', kernel_regularizer=l2(1)))
    model.add(Dense(1, activation='linear'))
    model.compile(loss='mse', optimizer='adam')
    return model


def get_random_forrest():
    return RandomForestRegressor(n_estimators=20, max_depth=1)


def detect_seasonality_and_trend(weekly_transportation_volumes, days_to_predict=6):
    """
    Detects seasonality and trend in time series data using the Gaussian process. Returns 2 pd.DataFrame, where the
    first DataFrame contains seasonality and trend for the given input data as estimated by the gaussian process and
    the second DataFrame contains the residues of subtracting seasonality and trend from the input data.

    :param weekly_transportation_volumes: pd.DataFrame as generated by get_weekly_transportation_volumes.
    Contains columns:
        - ds: datetime objects indicating the last day (sunday) of the corresponding week
        - y: total transportation volume of the week
    :param days_to_predict: Number of days to predict, defaults to 6.
    :return: pd.DataFrame that contains trend and seasonality, pd.DataFrame that contains the residues
    """

    gaussian_process = get_gaussian_process()
    number_of_observations = len(weekly_transportation_volumes)

    x_train = np.array(range(number_of_observations)).reshape(-1, 1)
    y_train = np.array(weekly_transportation_volumes['y'])

    gaussian_process.fit(x_train, y_train)
    print('gaussian process hyperparameters after optimization: ', gaussian_process.kernel_)

    x_predict = np.array(range(number_of_observations, number_of_observations + days_to_predict)).reshape(-1, 1)
    y_predict = gaussian_process.predict(x_predict)

    # create trend and seasonality as well as residues from model predictions on training data and prediction data
    trend_and_seasonality_train = gaussian_process.predict(x_train)
    residues_train = trend_and_seasonality_train - y_train
    trend_and_seasonality = list(trend_and_seasonality_train) + list(y_predict)
    residues = list(residues_train) + [np.nan for _ in range(days_to_predict)]

    # create DataFrames
    trend_and_seasonality_df = pd.DataFrame(trend_and_seasonality, columns=['trend_and_seasonality'])
    residue_df = pd.DataFrame(residues, columns=['residues'])

    return trend_and_seasonality_df, residue_df


def create_modelling_data(residues, prediction_distance=1):
    """
    Creates modelling data by shifting the residues. This allows to use standardized previous
    prices as input features.

    :param prediction_distance: this parameter defines how many weeks in advance we are trying to predict the residue
    :param residues: residues as generated by detect_seasonality_and_trend()
    :return: modelling_data
    """

    for i in range(prediction_distance, prediction_distance+20):
        residues['shift{}'.format(i)] = residues['residues'].shift(periods=i)

    # drop rows that include NAN values and reset index
    modelling_data = residues.dropna().reset_index(drop=True)

    return modelling_data


def generate_cv_sets(modelling_data):
    """
    Generates cross validation sets using forward chaining. This avoids data leakage, a common problem with
    most cross validation approaches when handling time series data.

    :param modelling_data: dataset as generated by create_modelling_data
    :return: generator that yields dictionaries of training_data and validation_data
    """

    for i in range(20, len(modelling_data), 10):
        training_data = modelling_data[i-20:i-10]
        validation_data = modelling_data[i-10:i]
        yield {'training_data': training_data, 'validation_data': validation_data}


def get_model_performance(model_factory, residues, prediction_distances_to_evaluate, do_final_evaluation=False):
    model_performance = {}

    for prediction_distance in prediction_distances_to_evaluate:
        model_performance[prediction_distance] = {}
        model_performance[prediction_distance]['train'] = {}
        model_performance[prediction_distance]['valid'] = {}
        modelling_data = create_modelling_data(residues.copy(), prediction_distance)
        train_and_valid_data = modelling_data[:-20]
        test_data = modelling_data[-20:]
        cv_split_generator = generate_cv_sets(train_and_valid_data)

        for i, cv_split in enumerate(cv_split_generator):
            # create a new model for each split
            model = model_factory()

            train_data = cv_split['training_data']
            valid_data = cv_split['validation_data']

            x_train = np.array(train_data.drop('residues', axis=1))
            y_train = np.array(train_data['residues']).reshape(-1,)

            x_valid = np.array(valid_data.drop('residues', axis=1))
            y_valid = np.array(valid_data['residues']).reshape(-1,)

            model.fit(x_train, y_train)
            train_predictions = model.predict(x_train)
            valid_predictions = model.predict(x_valid)

            split_train_performance = mean_absolute_error(y_train, train_predictions)
            model_performance[prediction_distance]['train'][i] = split_train_performance
            split_valid_performance = mean_absolute_error(y_valid, valid_predictions)
            model_performance[prediction_distance]['valid'][i] = split_valid_performance

        # get mean performance on train as well as validation sets
        model_performance[prediction_distance]['train']['mean_train_performance'] = \
            np.mean(list(model_performance[prediction_distance]['train'].values()))
        model_performance[prediction_distance]['valid']['mean_validation_performance'] = \
            np.mean(list(model_performance[prediction_distance]['valid'].values()))

        # final test on a holdout set can be used after models have been tuned to avoid overfitting to the
        # validation sets as generated by cross validation
        if do_final_evaluation:
            model = model_factory()

            x_train = np.array(train_and_valid_data.drop('residues', axis=1))
            y_train = np.array(train_and_valid_data['residues']).reshape(-1, )

            x_test = np.array(test_data.drop('residues', axis=1))
            y_test = np.array(test_data['residues']).reshape(-1,)

            model.fit(x_train, y_train)
            test_predictions = model.predict(x_test)
            test_performance = mean_absolute_error(y_test, test_predictions)

            model_performance[prediction_distance]['test_performance'] = test_performance

    return model_performance


def generate_performance_report(model_performances, prediction_distances_to_evaluate):
    """
    Generates a performance report for the specified models and prediction distances. Note: Test performance will only
    be filled if final evaluation is set to True during model performance evaluation.

    :param model_performances: a dict that contains model names as keys and their performance as
    generated by get_model_performance as values
    :param prediction_distances_to_evaluate: The prediction distances that should be part of the report. Must
    be subset of the distances used to generate the model performance
    :return: pd.DataFrame that reports the performance for the specified models. Contains train, validation and test
    performance for each prediction distance for each model.
    """

    report = {}
    for model_name, model_performance in model_performances.items():
        report[(model_name, 'train')] = \
            [model_performance[x]['train']['mean_train_performance'] for x in prediction_distances_to_evaluate]
        report[(model_name, 'valid')] = \
            [model_performance[x]['valid']['mean_validation_performance'] for x in prediction_distances_to_evaluate]
        report[(model_name, 'test')] = \
            [model_performance[x]['test_performance'] if 'test_performance' in model_performance[x]
             else np.NaN for x in prediction_distances_to_evaluate]
    report = pd.DataFrame(
        report,
        index=['MAE for {} days ahead prediction'.format(x) for x in prediction_distances_to_evaluate])
    return report


def get_final_predictions(residues, model_factory, prediction_distances_to_evaluate):
    predictions = []

    # the input for the final models will be the latest observations
    input_for_final_prediction = np.array(residues.dropna()[-20:]).reshape(1, -1)

    for prediction_distance in prediction_distances_to_evaluate:
        model = model_factory()
        modelling_data = create_modelling_data(residues.copy(), prediction_distance)
        x_train = np.array(modelling_data.drop('residues', axis=1))
        y_train = np.array(modelling_data['residues']).reshape(-1, )
        model.fit(x_train, y_train)
        predictions.append(model.predict(input_for_final_prediction))
    return predictions


def get_final_residuals(residues, final_predictions):
    final_residues = residues.copy()
    final_residues[-len(final_predictions):] = final_predictions
    return final_residues


#################
### EXECUTION ###
################

### DATA ENGINEERING ###

weekly_transportation_volumes = get_weekly_transportation_volumes('sample.csv')

### SEASONALITY AND TREND DETECTION ###

trend_and_seasonality, residues = detect_seasonality_and_trend(weekly_transportation_volumes)

### MODEL COMPARISONS ###

neural_network_factory = get_neural_network
random_forrest_factory = get_random_forrest
prediction_distances_to_evaluate = [1, 2, 3, 4, 5, 6]

neural_network_performance = \
    get_model_performance(neural_network_factory, residues, prediction_distances_to_evaluate, do_final_evaluation=True)
random_forrest_performance = \
    get_model_performance(random_forrest_factory, residues, prediction_distances_to_evaluate, do_final_evaluation=True)

performance_report = generate_performance_report(
    {'Neural Network': neural_network_performance,
     'Random Forrest': random_forrest_performance},
    prediction_distances_to_evaluate)

print('Performance Report: \n', performance_report)

### FINAL PREDICTIONS AND RESIDUES DATAFRAME ###

final_predictions = get_final_predictions(residues, neural_network_factory, prediction_distances_to_evaluate)
final_residues = get_final_residuals(residues, final_predictions)

